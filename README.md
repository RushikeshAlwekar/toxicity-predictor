# ğŸš¦ ToxicMeter â€“ Toxicity Predictor

**ToxicMeter** is a full-stack web application that predicts the toxicity level of user-entered text. It analyzes text for various toxic traits such as insults, threats, and identity-based hate using a trained machine learning model. The goal is to provide users with real-time feedback on the toxicity of their input in an intuitive and visually engaging format.

---

## ğŸ” Overview

This application allows users to enter any sentence or paragraph, and instantly receive a breakdown of how toxic the text is. The output highlights different toxicity categories with color-coded indicators â€” making it simple to understand and interpret.

The app uses a trained machine learning model behind the scenes to make accurate predictions. It has two main parts:

- A **frontend** built with HTML, CSS, and JavaScript
- A **backend** powered by **Flask (Python)** that processes input and returns predictions from a pre-trained model

---

## âœ¨ Key Features

- Supports **six toxicity categories**:
  - `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, `identity_hate`
- Displays **percent scores** for each category
- Simple and clean **user interface**
- Color-coded output:
  - ğŸŸ¢ Low (<30%)
  - ğŸŸ¡ Medium (30â€“70%)
  - ğŸ”´ High (>70%)
- Tag for **safe input** if all scores are low

---

## âš™ï¸ How It Works

1. The user types text into the input box.
2. On clicking **"Analyze"**, the text is sent to the Flask backend via a POST request.
3. The backend:
   - Cleans the input text
   - Converts it into TF-IDF vectors
   - Uses a trained logistic regression model to make predictions
4. The result is sent back to the frontend and visualized for the user.

---

## ğŸ› ï¸ Project Structure

toxicity-predictor/
â”œâ”€â”€ app.py # Flask backend (API)
â”œâ”€â”€ index.html # Frontend interface
â”œâ”€â”€ styles.css # Styling
â”œâ”€â”€ script.js # JS logic (API calls + DOM updates)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ train.py # Script to train the ML model
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ toxicity_model.pkl # Saved trained model
â”‚ â””â”€â”€ preprocessing.py # Text cleaning function
â””â”€â”€ README.md # You're reading it!

markdown
Copy
Edit

---

## ğŸ§  Training the Model

The machine learning model used in this project was trained on the **Jigsaw Toxic Comment Classification** dataset from Kaggle. It is a multi-label classification problem where each comment can belong to more than one toxicity category.

The training steps (in `train.py`) are:

- Load and preprocess the dataset
- Clean the text (remove punctuation, lowercase, lemmatize)
- Vectorize text using **TF-IDF**
- Train a `OneVsRestClassifier` with **Logistic Regression**
- Save the trained model as `toxicity_model.pkl`

To retrain the model:

```bash
python train.py
ğŸ“¦ Technologies Used
ğŸ”¹ Frontend
HTML5

CSS3

JavaScript (Vanilla)

ğŸ”¹ Backend
Python 3

Flask

Flask-CORS

scikit-learn

NLTK

joblib
```
